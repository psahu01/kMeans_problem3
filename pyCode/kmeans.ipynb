{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e2073f34262e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Read the foods file and filter out text but review/text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mtestFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../test.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mreviewStr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestFile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../test.txt'"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# CIS-563 Introduction to Data Science\n",
    "# Homowork2\n",
    "# Problem 3\n",
    "# Prateek Sahu (801311241)\n",
    "#######################################\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def removePunctuations(s):\n",
    "    return re.sub(r'['+string.punctuation+']', ' ', s)\n",
    "\n",
    "def removeBogusCharacters(s):\n",
    "    return re.sub(r\"br\\s|\\d\", '', s)\n",
    "\n",
    "# Returns the set of stopWords\n",
    "def stopWordsSet():\n",
    "    with open(\"../stopWords.txt\", \"r\") as f:\n",
    "        return set(removePunctuations(f.read()).split())\n",
    "        #return set(f.read().splitlines())\n",
    "\n",
    "# Read the foods file and filter out text but review/text\n",
    "testFile = open(\"../foods.txt\",\"r\", encoding='latin-1')\n",
    "reviewStr = ''\n",
    "for line in testFile:\n",
    "    if line.startswith('review/text: '):\n",
    "        reviewStr = reviewStr + line.lstrip('review/text: ')\n",
    "testFile.close()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Data Pre-processing(Removing punctuations, Bogus Characters and numbers)\n",
    "#-------------------------------------------------------------------------\n",
    "cleanReviewStr = removePunctuations(removeBogusCharacters(reviewStr.lower()))\n",
    "reviewList = cleanReviewStr.splitlines();\n",
    "reviewWordsList = cleanReviewStr.split()\n",
    "\n",
    "L = set(reviewWordsList)\n",
    "W = L - stopWordsSet();\n",
    "\n",
    "reviewWordsFreq = Counter(reviewWordsList)\n",
    "for n in list(stopWordsSet()):\n",
    "    del reviewWordsFreq[n]\n",
    "\n",
    "# Write top 500 words with its frequency in file\n",
    "topWordsList = reviewWordsFreq.most_common(500)\n",
    "topWordsDict = dict(topWordsList)\n",
    "with open(r\"../top500Words.txt\", 'w') as file:\n",
    "    file.write(str(topWordsDict))\n",
    "\n",
    "#-------------\n",
    "#Vectorization\n",
    "#-------------\n",
    "corpus = np.array(list(zip(*topWordsList)))[0]\n",
    "\n",
    "df = pd.DataFrame(reviewList)\n",
    "rawDoc = df.iloc[0:,0]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "P = vectorizer.transform(rawDoc)\n",
    "sparseMatrix = P.A\n",
    "\n",
    "#-------------\n",
    "# K-Means\n",
    "#-------------\n",
    "k = 10\n",
    "model = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(sparseMatrix)\n",
    "\n",
    "centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "#get the date in the cluster and sort them to get top 5 data values\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(k):\n",
    "    print(\"Cluster {}:\".format(i+1))\n",
    "    for data in centroids[i, :5]:\n",
    "        print(\"{}: {}\".format(terms[data], model.cluster_centers_[i,data]))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
